{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CLASSIFICATION PROJECT - PREDICTING EMPLOYEE ATTRITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BUSINESS UNDERSTANDING\n",
    "\n",
    "Employee attrition refers to the process where employees leave an organization, either voluntarily or involuntarily. High attrition rates can be costly for businesses, as they impact productivity, morale, recruitment costs, and training expenses. \n",
    "The primary objective of predicting employee attrition is to identify employees who are at risk of leaving the organization in the near future. By doing so, companies can take proactive measures to improve retention, enhance employee satisfaction, and reduce the overall cost of turnover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PROJECT GOAL\n",
    "The goal of this project is to develop a robust machine learning pipeline to predict whether specific employees are likely to leave the company. The predictive modeling will be conducted following an in-depth analysis of the dataset obtained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANALYTICAL QUESTIONS\n",
    "1. What is the percentage of Attrition?\n",
    "2. How satisfied are employees after 3 years at the company?\n",
    "3. Does marital status affect attrition rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA UNDERSTANDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Features\n",
    "* Age: Age of employees\n",
    "\n",
    "* Attrition: Employee attrition status\n",
    "\n",
    "* Department: Department of employees\n",
    "\n",
    "* Education Level: 1-Bachelor's degree; 2- Master's degree\n",
    "\n",
    "* EducationField\n",
    "\n",
    "* Environment Satisfaction: 1-Low; 2-Medium; 3-High; 4-Very High;\n",
    "\n",
    "* Job Satisfaction: 1-Low; 2-Medium; 3-High; 4-Very High;\n",
    "\n",
    "* MaritalStatus\n",
    "\n",
    "* Gross Salary\n",
    "\n",
    "* Work Life Balance: 1-Bad; 2-Good; 3-Better; 4-Best;\n",
    "\n",
    "* Length of Service: number of years with employer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#Data Preparation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "data = pd.read_excel('Data/Attrition Dataset.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check cell values\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All the columns have the correct data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "* Most of the columns in the dataset do not have missing values.\n",
    "* The column \"Gross Salary\" has 3 missing values, meaning that there are 3 records where the \"Gross Salary\" information is not provided. These will be handled using imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the Employee ID column\n",
    "data = data.drop('Employee ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='object').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicates\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This implies there are no duplicates in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values for each column\n",
    "for column in data.columns:\n",
    "    print(f'Column Name: {column}\\n')\n",
    "    print(f'Number of Unique Values: {data[column].unique().size}\\n')\n",
    "    print(f'{data[column].unique()}')\n",
    "    print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert column names to lower case\n",
    "data.columns = data.columns.str.lower()\n",
    "\n",
    "#Check the columns to confirm\n",
    "column_names = data.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge column names\n",
    "data.columns = data.columns.str.replace(' ', '_')\n",
    "\n",
    "# Check the new column names\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename educationfield column\n",
    "data.rename(columns={'educationfield': 'education_field'}, inplace=True)\n",
    "\n",
    "# Check the updated column names\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting color palette for the project\n",
    "sns.color_palette(\"pastel\")\n",
    "data.hist(figsize=(10, 10), grid=False, color='skyblue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add KDE plots to see a smoother representation of the distribution of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.select_dtypes('number').columns:\n",
    "    data[column].plot(kind='kde')\n",
    "    plt.title(f'KDE for {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add a Boxplot to detect outliers and the scale of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=data, orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Points above the upper whisker indicate salaries much higher than typical, suggesting significant variation in gross salaries within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-variate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = data.corr(numeric_only=True)\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data=correlation, annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['attrition', 'length_of_service', 'age']]\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.pairplot(df, hue='attrition', palette={'Yes': 'skyblue', 'No': 'lightgreen'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answering Analytical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the percentage of Attrition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "data['attrition'].value_counts().plot.pie(startangle=90, colors=['pink', 'skyblue'], autopct='%1.1f%%', explode=(0.01, 0.05), pctdistance=0.85)\n",
    "plt.title('The Attrition distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How satisfied are employees after 3 years at the company?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['job_satisfaction', 'attrition'])['department'].count().rename('Total').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_three_years = data[data['length_of_service'] > 3]\n",
    "after_three_years.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_three_years[['job_satisfaction', 'attrition']].groupby(['job_satisfaction', 'attrition'])['attrition'].count().rename('Count').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does marital status affect attrition rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total count and attrition count by marital status\n",
    "marital_status_summary = data.groupby('marital_status')['attrition'].value_counts().unstack().fillna(0)\n",
    "\n",
    "# Calculate the attrition rate\n",
    "marital_status_summary['attrition_rate'] = marital_status_summary['Yes'] / (marital_status_summary['Yes'] + marital_status_summary['No'])\n",
    "\n",
    "# Reset index for better readability\n",
    "marital_status_summary = marital_status_summary.reset_index()\n",
    "\n",
    "print(marital_status_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total count of attrition (Yes + No) for each marital status\n",
    "marital_status_summary['Total'] = marital_status_summary['Yes'] + marital_status_summary['No']\n",
    "\n",
    "# Calculate the percentage of Yes and No attrition\n",
    "marital_status_summary['Yes_pct'] = marital_status_summary['Yes'] / marital_status_summary['Total'] * 100\n",
    "marital_status_summary['No_pct'] = marital_status_summary['No'] / marital_status_summary['Total'] * 100\n",
    "\n",
    "# Create a stacked bar chart to visualize the percentages of 'Yes' and 'No' attrition by marital status\n",
    "plt.figure(figsize=(10, 6))\n",
    "marital_status_summary[['Yes_pct', 'No_pct']].plot(kind='bar', stacked=True, color=['skyblue', 'lightgreen'], figsize=(10, 6))\n",
    "\n",
    "# Add percentage labels on each bar\n",
    "for i in range(len(marital_status_summary)):\n",
    "    plt.text(i, marital_status_summary['Yes_pct'].iloc[i] / 2, f\"{marital_status_summary['Yes_pct'].iloc[i]:.1f}%\", ha='center', color='white', fontweight='bold')\n",
    "    plt.text(i, marital_status_summary['Yes_pct'].iloc[i] + marital_status_summary['No_pct'].iloc[i] / 2, f\"{marital_status_summary['No_pct'].iloc[i]:.1f}%\", ha='center', color='white', fontweight='bold')\n",
    "\n",
    "plt.title('Percentage of Attrition by Marital Status')\n",
    "plt.xlabel('marital_status')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xticks(ticks=range(len(marital_status_summary)), labels=marital_status_summary['marital_status'], rotation=0)\n",
    "plt.legend(['Attrition: Yes', 'Attrition: No'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check cell values\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The missing values in the gross_salary column will be replaced with the median of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'gross_salary' with the median\n",
    "median_gross_salary = data['gross_salary'].median()\n",
    "data['gross_salary'].fillna(median_gross_salary, inplace=True)\n",
    "\n",
    "# Check if the missing values are filled\n",
    "print(data['gross_salary'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check cell values\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if Dataset is Balanced\n",
    "To check if the dataset is balanced, we set a threshold of 5%. If the absolute difference between the counts of the two classes is less than the threshold, then the dataset is considered balanced; otherwise, it's considered imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each class\n",
    "class_counts = data['attrition'].value_counts()\n",
    "\n",
    "# Set the threshold for imbalance (5% of the total number of rows)\n",
    "threshold = len(data) * 0.05 \n",
    "\n",
    "# Check if the dataset is balanced using .iloc to access by position\n",
    "is_balanced = abs(class_counts.iloc[0] - class_counts.iloc[1]) < threshold\n",
    "\n",
    "if is_balanced:\n",
    "    print(\"The dataset is balanced.\")\n",
    "else:\n",
    "    print(\"The dataset is imbalanced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the occurrences of each class\n",
    "class_counts = data['attrition'].value_counts()\n",
    "\n",
    "#Plot the distribution of the target variable\n",
    "plt.figure(figsize=(6, 4))\n",
    "bars = class_counts.plot(kind='bar', color=['skyblue', 'orange'])\n",
    "plt.title('Distribution of Attrition')\n",
    "plt.xlabel('Attrition')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "#Annotate the bars with churn counts\n",
    "for i, count in enumerate(class_counts):\n",
    "    plt.text(i, count + 10, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The visual also confirms the dataset is balanced since the absolute difference between the classes is less than 50 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODELING AND EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Balanced Dataset\n",
    "In splitting the data, it is done such that;\n",
    "X contains all the features except the target variable (attrition).\n",
    "\n",
    "y contains only the target variable (attrition).\n",
    "\n",
    "We use train_test_split to split the data into training and evaluation sets and set test_size to 0.3 which specifies that 30% of the data should be used for evaluation, while the rest is used for training.\n",
    "\n",
    "X_train and y_train contain the training features and target variable respectively.X_eval and y_eval contain the evaluation features and target variable respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define features (X) and target variable (y)\n",
    "X = data.drop('attrition', axis=1) \n",
    "y = data['attrition']  \n",
    "\n",
    "#Split the dataset into training and evaluation sets\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "#Encode y_train\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "\n",
    "#Encode y_test\n",
    "y_eval_encoded = encoder.transform(y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get categorical columns\n",
    "categorical_columns = X.select_dtypes('object').columns\n",
    "\n",
    "#Get numerical columns\n",
    "numerical_columns = X.select_dtypes('number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare numerical pipeline\n",
    "numerical_pipeline=Pipeline(steps=[\n",
    "('numerical_imputer',SimpleImputer(strategy='median')),\n",
    "('scaler', StandardScaler())\n",
    "    \n",
    "])\n",
    "\n",
    "#Prepare categorical pipeline\n",
    "categorical_pipeline=Pipeline(steps=[\n",
    "    ('categorical_imputer',SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "\n",
    "])\n",
    "\n",
    "#Column transformer preparation\n",
    "preprocessor=ColumnTransformer(transformers=[\n",
    "    ('numerical_pipeline', numerical_pipeline,numerical_columns),\n",
    "    ('categorical_pipeline', categorical_pipeline, categorical_columns)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the models\n",
    "model = [\n",
    "    ('K-Nearest_Neighbors', KNeighborsClassifier(n_neighbors=5)),  \n",
    "    ('Logistic_Regression', LogisticRegression(random_state=42)),  \n",
    "    ('Support_Vector_Machine', SVC(random_state=42)),  \n",
    "    ('Decision_Tree', DecisionTreeClassifier(random_state=42)),  \n",
    "    ('Random_Forest', RandomForestClassifier(random_state=42)),  \n",
    "    ('Gradient_Boosting', GradientBoostingClassifier(random_state=42)),  \n",
    "]\n",
    "\n",
    "# Initialize an empty dictionary to store pipelines\n",
    "all_pipelines = {}\n",
    "\n",
    "#Create a DataFrame for the metrics\n",
    "metrics_output = pd.DataFrame(columns=['model_name', 'accuracy', 'precision', 'recall', 'f1_score'])\n",
    "\n",
    "#Train and evaluate each model\n",
    "for model_name, classifier in model:\n",
    "    \n",
    "    #Create pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selection', SelectKBest(mutual_info_classif, k='all')),\n",
    "        ('classifier', classifier),\n",
    "    ])\n",
    "    \n",
    "    #Fit data to pipeline\n",
    "    pipeline.fit(X_train, y_train_encoded)\n",
    "    all_pipelines[model_name] = pipeline\n",
    "\n",
    "    #Make predictions on the test set\n",
    "    y_pred = pipeline.predict(X_eval)\n",
    "\n",
    "    #Generate classification report for each model\n",
    "    metrics = classification_report(y_eval_encoded, y_pred, output_dict=True)\n",
    "    \n",
    "    #Evaluate the model\n",
    "    accuracy = metrics['accuracy']\n",
    "    precision = metrics['weighted avg']['precision']\n",
    "    recall = metrics['weighted avg']['recall']\n",
    "    f1_score= metrics['weighted avg']['f1-score']\n",
    "\n",
    "    #Add metrics to metrics_output\n",
    "    metrics_output.loc[len(metrics_output)] = [model_name, accuracy, precision, recall, f1_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the metrics_output\n",
    "metrics_output.sort_values(ascending=False, by='f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "* Support Vector Machine (SVM) shows the best performance in terms of all metrics, making it a solid candidate for the highest-performing model in the set.\n",
    "\n",
    "* Logistic Regression and Gradient Boosting perform similarly, providing reasonable accuracy with slightly lower scores than SVM, but may still be viable options. \n",
    "\n",
    "* Decision Tree and Random Forest have the lowest accuracy and other metrics, which indicates they may need tuning or might not be well-suited for this dataset.\n",
    "\n",
    "* The F1-score for all models is generally close to the accuracy, indicating balanced precision and recall for most models, with SVM having the best balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a Confusion Matrix for the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the models\n",
    "model = [\n",
    "    ('K-Nearest_Neighbors', KNeighborsClassifier(n_neighbors=5)),  \n",
    "    ('Logistic_Regression', LogisticRegression(random_state=42)),  \n",
    "    ('Support_Vector_Machine', SVC(random_state=42)),  \n",
    "    ('Decision_Tree', DecisionTreeClassifier(random_state=42)),  \n",
    "    ('Random_Forest', RandomForestClassifier(random_state=42)),  \n",
    "    ('Gradient_Boosting', GradientBoostingClassifier(random_state=42)),  \n",
    "]\n",
    "\n",
    "# Initialize an empty dictionary to store pipelines\n",
    "all_pipelines_c = {}\n",
    "\n",
    "# All confusion matrix\n",
    "all_confusion_matrix =  {}\n",
    "\n",
    "#Create a DataFrame for the metrics\n",
    "c_metrics_output = pd.DataFrame(columns=['model_name', 'accuracy', 'precision', 'recall', 'f1_score'])\n",
    "\n",
    "#Train and evaluate each model\n",
    "for model_name, classifier in model:\n",
    "    \n",
    "    #Create pipeline\n",
    "    c_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier),\n",
    "    ])\n",
    "    \n",
    "    #Fit data to pipeline\n",
    "    c_pipeline.fit(X_train, y_train_encoded)\n",
    "    all_pipelines_c[model_name] = c_pipeline\n",
    "\n",
    "    #Make predictions on the test set\n",
    "    y_pred = c_pipeline.predict(X_eval)\n",
    "\n",
    "    #Generate classification report for each model\n",
    "    c_metrics = classification_report(y_eval_encoded, y_pred, output_dict=True)\n",
    "    \n",
    "    #Evaluate the model\n",
    "    c_accuracy = c_metrics['accuracy']\n",
    "    c_precision = c_metrics['weighted avg']['precision']\n",
    "    c_recall = c_metrics['weighted avg']['recall']\n",
    "    c_f1_score= c_metrics['weighted avg']['f1-score']\n",
    "\n",
    "    #Add metrics to metrics_output\n",
    "    c_metrics_output.loc[len(c_metrics_output)] = [model_name, c_accuracy, c_precision, c_recall, c_f1_score]\n",
    "\n",
    "    # Compute the confusion matrix and store it\n",
    "    c_matrix = confusion_matrix(y_eval_encoded, y_pred)\n",
    "    all_confusion_matrix[model_name] = c_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the keys (model names) in the all_confusion_matrix dictionary\n",
    "for model_name, c_matrix in all_confusion_matrix.items():\n",
    "    print(f\"Confusion Matrix for {model_name}:\")\n",
    "    print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices using heatmaps\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for i, (model, c_matrix) in enumerate(all_confusion_matrix.items()):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    sns.heatmap(c_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "    ax.set_title(f\"Confusion Matrix for {model}\")\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_ylabel('True label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "* K-Nearest Neighbors and Support Vector Machine both show relatively high false negatives, meaning they are more likely to miss true positives.\n",
    "\n",
    "* Logistic Regression and Gradient Boosting have a more balanced error rate between false positives and false negatives, performing slightly better overall.\n",
    "\n",
    "* Random Forest and Decision Tree also perform similarly but tend to have higher false negatives, especially Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Evaluation Using ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "model = [\n",
    "    ('K-Nearest_Neighbors', KNeighborsClassifier(n_neighbors=5)),  \n",
    "    ('Logistic_Regression', LogisticRegression(random_state=42)),  \n",
    "    ('Support_Vector_Machine', SVC(random_state=42, probability=True)),  # Set probability=True\n",
    "    ('Decision_Tree', DecisionTreeClassifier(random_state=42)),  \n",
    "    ('Random_Forest', RandomForestClassifier(random_state=42)),  \n",
    "    ('Gradient_Boosting', GradientBoostingClassifier(random_state=42)),  \n",
    "]\n",
    "\n",
    "all_pipelines_c = {}\n",
    "\n",
    "# Create a DataFrame for the metrics\n",
    "c_metrics_output = pd.DataFrame(columns=['model_name', 'accuracy', 'precision', 'recall', 'f1_score'])\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, classifier in model:\n",
    "    \n",
    "    # Create pipeline with feature selection\n",
    "    c_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selection', SelectKBest(score_func=f_classif, k=5)),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    # Fit data to pipeline\n",
    "    c_pipeline.fit(X_train, y_train_encoded)\n",
    "    all_pipelines_c[model_name] = c_pipeline\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = c_pipeline.predict(X_eval)\n",
    "\n",
    "    # Generate classification report for each model\n",
    "    c_metrics = classification_report(y_eval_encoded, y_pred, output_dict=True)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    c_accuracy = c_metrics['accuracy']\n",
    "    c_precision = c_metrics['weighted avg']['precision']\n",
    "    c_recall = c_metrics['weighted avg']['recall']\n",
    "    c_f1_score= c_metrics['weighted avg']['f1-score']\n",
    "\n",
    "    # Add metrics to metrics_output\n",
    "    c_metrics_output.loc[len(c_metrics_output)] = [model_name, c_accuracy, c_precision, c_recall, c_f1_score]\n",
    "\n",
    "# Plot roc_curves\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "all_roc_data = {}\n",
    "\n",
    "for model_name, c_pipeline in all_pipelines_c.items():\n",
    "\n",
    "    y_score = c_pipeline.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_eval_encoded, y_score)\n",
    "    \n",
    "    roc_auc = auc(fpr,tpr)\n",
    "\n",
    "    roc_data = pd.DataFrame({'False Positive Rate': fpr, 'True Positive Rate': tpr, 'Threshold': thresholds})\n",
    "\n",
    "    all_roc_data[model_name] = roc_data\n",
    "\n",
    "    ax.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc})')\n",
    "\n",
    "    ax.plot([0,1], [0,1])\n",
    "\n",
    "    ax.set_ylabel('False Positive Rate')\n",
    "    \n",
    "    ax.set_xlabel('True Positive Rate')\n",
    "\n",
    "\n",
    "ax.set_title('ROC Curve Plot for all Pipelines')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based on the provided AUC (Area Under the ROC Curve) values, the best performing models are;\n",
    "1. Gradient Boosting\n",
    "2. Logistic Regression\n",
    "3. Random_Forest\n",
    "\n",
    "* These models have higher AUC values, indicating better overall performance in terms of the trade-off between true positive rate and false positive rate. \n",
    "* We will select Gradient Boosting and Logistic Regression as the best performing models, and perform threshold optimization as well as hyperparameter tunin on these two models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of the threshold for Gradient Boosting\n",
    "all_roc_data['Gradient_Boosting'].loc[10:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview the Gradient Boosting pipeline\n",
    "GradientBoostingClassifier_pipeline = all_pipelines['Gradient_Boosting']\n",
    "GradientBoostingClassifier_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best threshold\n",
    "gradient_threshold = 0.2\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = GradientBoostingClassifier_pipeline.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# Make predictions based on the threshold\n",
    "predictions = (y_pred_proba > gradient_threshold).astype(int)\n",
    "\n",
    "# Compute confusion matrix\n",
    "gradient_threshold_matrix = confusion_matrix(y_eval_encoded, predictions)\n",
    "\n",
    "# Saving the best model and threshold in variables\n",
    "best_gradient_boosting_model = GradientBoostingClassifier_pipeline\n",
    "best_gradient_threshold = gradient_threshold\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(gradient_threshold_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(gradient_threshold_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Gradient Boosting')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of the threshold for Gradient Boosting\n",
    "all_roc_data['Logistic_Regression'].loc[10:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview the logistic regression pipeline\n",
    "LogisticRegression_pipeline = all_pipelines['Logistic_Regression']\n",
    "LogisticRegression_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best threshold\n",
    "LR_threshold = 0.2\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = LogisticRegression_pipeline.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# Make predictions based on the threshold\n",
    "predictions = (y_pred_proba > LR_threshold).astype(int)\n",
    "\n",
    "# Compute confusion matrix\n",
    "LR_threshold_matrix = confusion_matrix(y_eval_encoded, predictions)\n",
    "\n",
    "# Saving the best model and threshold in variables\n",
    "best_logistic_regression_model = LogisticRegression_pipeline\n",
    "best_LR_threshold = LR_threshold\n",
    "LR_threshold_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(LR_threshold_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HYPERPARAMETER TUNING TO IMPROVE MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform hyperparameter tuning on the two best performing models to improve their performance thus Gradient Boosting and Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypertuning Gradient Boosting Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the saved GradientBoostingClassifier pipeline\n",
    "current_params = best_gradient_boosting_model.get_params()\n",
    "current_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],   # Number of trees\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.05],  # Learning rate\n",
    "    'classifier__max_depth': [3, 5, 7],  # Depth of trees\n",
    "    'classifier__min_samples_split': [2, 5, 10],  # Minimum samples for node split\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],  # Minimum samples at leaf\n",
    "    'classifier__subsample': [0.8, 1.0],  # Fraction of samples used for training each tree\n",
    "}\n",
    "\n",
    "# Create the pipeline (your existing pipeline)\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # ColumnTransformer for preprocessing\n",
    "    ('feature_selection', SelectKBest(score_func=mutual_info_classif, k='all')),  # Feature selection\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))  # Classifier\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV with the pipeline and parameter grid\n",
    "grid_search = GridSearchCV(estimator=pipeline, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5,  # 5-fold cross-validation\n",
    "                           scoring='accuracy',  # Evaluation metric\n",
    "                           verbose=2,  # Print detailed progress\n",
    "                           n_jobs=-1)  # Use all available cores\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Get the best parameters and best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Hyperparameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store the results\n",
    "tuned_models_df = pd.DataFrame(columns=['Model name', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "all_pipelines_c[model_name] = best_model\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_eval)\n",
    "\n",
    "# Store classification report values as a dictionary\n",
    "tuned_metrics = classification_report(y_eval_encoded, y_pred, output_dict=True)\n",
    "\n",
    "# Grab values from the metric dictionary\n",
    "accuracy = tuned_metrics['accuracy']\n",
    "precision = tuned_metrics['weighted avg']['precision']\n",
    "recall = tuned_metrics['weighted avg']['recall']\n",
    "f1 = tuned_metrics['weighted avg']['f1-score']\n",
    "    \n",
    "# Add these values to the table\n",
    "tuned_models_df.loc[len(tuned_models_df)] = [model_name, accuracy, precision, recall, f1]\n",
    "\n",
    "# Sort table to have highest f1 on top\n",
    "tuned_models_df.sort_values(by='F1-Score', ascending=False, inplace=True)\n",
    "\n",
    "#Display the results\n",
    "print(tuned_models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the tuned Gradient Boosting model to determine the trade-offs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best threshold\n",
    "gradient_threshold = 0.2\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = best_model.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# Make predictions based on the threshold\n",
    "predictions = (y_pred_proba > gradient_threshold).astype(int)\n",
    "\n",
    "# Compute confusion matrix\n",
    "gradient_threshold_matrix = confusion_matrix(y_eval_encoded, predictions)\n",
    "\n",
    "# Saving the best model and threshold in variables\n",
    "best_gradient_boosting_model = best_model\n",
    "best_gradient_threshold = gradient_threshold\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(gradient_threshold_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(gradient_threshold_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Gradient Boosting')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "* The original model generally performs better than the tuned model across all key metrics (accuracy, precision, and F1-score), with recall being identical for both models. This suggests that the hyperparameter tuning didn't improve the model's performance in this case. But it's essential to consider the trade-offs between these metrics based on the specific objectives of the Project.\n",
    "\n",
    "* Confusion Matrix for the original gradient boosting model has the best overall performance, with the highest accuracy, precision, recall, and F1 score. It has a higher Recall, meaning the model is better at identifying positive cases (fewer false negatives).\n",
    "\n",
    "* Confusion Matrix for the tuned model has good recall (75%), meaning it captures most of the actual positives, but it has a lower precision (53%), meaning many of the predicted positives are actually false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypertuning the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Column transformer preparation\n",
    "preprocessor=ColumnTransformer(transformers=[\n",
    "    ('numerical_pipeline', numerical_pipeline,numerical_columns),\n",
    "    ('categorical_pipeline', categorical_pipeline, categorical_columns)\n",
    "])\n",
    "\n",
    "# Define the parameter distribution for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'classifier__C': np.logspace(-4, 4, 20),  \n",
    "    'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'classifier__solver': ['lbfgs', 'liblinear', 'saga'],  \n",
    "    'classifier__max_iter': [100, 200, 300], }\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Initialize RandomizedSearchCV with the pipeline and parameter distribution\n",
    "random_search = RandomizedSearchCV(estimator=pipeline,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=50,  \n",
    "                                   cv=5,  \n",
    "                                   scoring='accuracy',  \n",
    "                                   verbose=2, \n",
    "                                   n_jobs=-1, \n",
    "                                   random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Get the best parameters and best model\n",
    "best_params_l = random_search.best_params_\n",
    "best_model_l = random_search.best_estimator_\n",
    "\n",
    "print(\"Best Hyperparameters: \", best_params_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store the results\n",
    "tuned_models_df_l = pd.DataFrame(columns=['Model name', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "# Store the model name\n",
    "model_name = \"Logistic Regression\"  \n",
    "\n",
    "best_model_l = random_search.best_estimator_  \n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model_l.predict(X_eval)\n",
    "\n",
    "# Store classification report values as a dictionary\n",
    "tuned_metrics_l = classification_report(y_eval_encoded, y_pred, output_dict=True)\n",
    "\n",
    "# Grab values from the metric dictionary\n",
    "accuracy = tuned_metrics_l['accuracy']\n",
    "precision = tuned_metrics_l['weighted avg']['precision']\n",
    "recall = tuned_metrics_l['weighted avg']['recall']\n",
    "f1 = tuned_metrics_l['weighted avg']['f1-score']\n",
    "    \n",
    "# Add these values to the DataFrame\n",
    "tuned_models_df_l.loc[len(tuned_models_df_l)] = [model_name, accuracy, precision, recall, f1]\n",
    "\n",
    "# Sort table to have highest F1 on top\n",
    "tuned_models_df_l.sort_values(by='F1-Score', ascending=False, inplace=True)\n",
    "\n",
    "# Display the results\n",
    "print(tuned_models_df_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the tuned Logistic Regression Model to determine the trade-offs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best threshold\n",
    "LR_threshold = 0.2\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = best_model_l.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# Make predictions based on the threshold\n",
    "predictions = (y_pred_proba > LR_threshold).astype(int)\n",
    "\n",
    "# Compute confusion matrix\n",
    "LR_threshold_matrix = confusion_matrix(y_eval_encoded, predictions)\n",
    "\n",
    "# Saving the best model and threshold in variables\n",
    "best_logistic_regression_model = best_model_l\n",
    "best_LR_threshold = LR_threshold\n",
    "LR_threshold_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(LR_threshold_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "* The original Logistic Regression model performs better across all metrics (accuracy, precision, recall, and F1-score) than the tuned logistic regression model, making the original model preferable based on these results.\n",
    "\n",
    "* However, the confusion matrix for the tuned model  performs better at identifying positive cases (higher TP) and reducing false negatives (FN) which is beneficial to this project. \n",
    "\n",
    "* The original matrix performs better in terms of identifying negative cases (higher TN) and reducing false positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROJECT IMPACT ASSESSMENT\n",
    "##### Project Objective\n",
    "The main objective of the project is to create a predictive model that improves true positives and reduces the occurrence of false negatives. Specifically, in the case of employee attrition prediction, a false negative arises when the model inaccurately forecasts that an employee will not exit, whereas they indeed do. This error could lead to missed chances to take action and retain valuable employees.\n",
    "\n",
    "##### Optimal Model Selection\n",
    "Due to the paramount importance of reducing false negatives, the ideal choice for best model is the model that shows the fewest occurrences of false negatives. In our assessment, the Gradient Boosting model has proven to excel in this aspect. The decrease in false negatives indicates an enhanced ability to recognize employees prone to exit, aligning closely with our key project objectives.\n",
    "\n",
    "##### Conclusion\n",
    "In conclusion, the Gradient Boosting model emerges as the top choice for our attrition prediction task, emphasizing the minimization of false negatives. Adopting this model empowers businesses to preemptively pinpoint and retain employees prone to exiting, ultimately bolstering employee retention rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVE THE MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the best model and threshold using joblib\n",
    "joblib.dump((best_gradient_boosting_model, best_gradient_threshold), 'best_gb_model_and_threshold.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the encoder\n",
    "joblib.dump(encoder, 'Model/encoder.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
